<!DOCTYPE html><!--9hS1bw4vLqB3Hrxjps2LV--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/886f446b96dc7734-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/9cf9c6e84ed13b5e-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/e693e841d50dcf2f-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/a7d7cda7084e7e71.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d36f1d6244358adb.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-ebd51be49873d76c.js" async=""></script><script src="/_next/static/chunks/main-app-1e23de75a97db036.js" async=""></script><script src="/_next/static/chunks/850-de350858a22819d2.js" async=""></script><script src="/_next/static/chunks/app/layout-30653af3c87d6211.js" async=""></script><script src="/_next/static/chunks/app/publications/page-b592bbbaf126a7f6.js" async=""></script><meta name="next-size-adjust" content=""/><title>Create Next App</title><meta name="description" content="Generated by create next app"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_188709 __variable_9a8899 __variable_7dbc8c __className_e73cbf antialiased relative"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("data-theme","theme","system",null,["light","dark"],null,true,true)</script><div class="flex justify-center w-full fixed top-0 left-0 bg-white z-[999] shadow-lg"><div class=" xl:max-w-[1140px] 2xl:max-w-[1320px] lg:max-w-[960px] md:max-w-[720px] sm:max-w-[540px] flex items-center  md:justify-start justify-between px-4 md:px-0 w-full h-[60px]"><div class="flex items-end justify-start gap-2"><img alt="CBIM Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="" style="color:transparent" srcSet="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=48&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=96&amp;q=75 2x" src="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=96&amp;q=75"/><div class="border-l-2 pl-2  text-3xl font-bold leading-none">CBIM</div></div><div class="hidden items-center justify-start h-full md:flex"><a style="pointer-events:auto" class="ml-8 text-lg font-bold tracking-wider h-full " href="/"><div class="relative h-full flex items-center justify-center group"><div class="">Home</div><div class="
                w-0 group-hover:w-[70px]
                transition-all duration-300 ease-in-out
               h-[5px] absolute bottom-0 left-1/2 -translate-x-1/2 bg-primary-red
              "></div></div></a><a style="pointer-events:auto" class="ml-8 text-lg font-bold tracking-wider h-full " href="/members"><div class="relative h-full flex items-center justify-center group"><div class="">Members</div><div class="
                w-0 group-hover:w-[70px]
                transition-all duration-300 ease-in-out
               h-[5px] absolute bottom-0 left-1/2 -translate-x-1/2 bg-primary-red
              "></div></div></a><a style="pointer-events:auto" class="ml-8 text-lg font-bold tracking-wider h-full " href="/publications"><div class="relative h-full flex items-center justify-center group"><div class=" text-primary-red">Publications</div><div class="
                w-[70px] text-primary-red
                transition-all duration-300 ease-in-out
               h-[5px] absolute bottom-0 left-1/2 -translate-x-1/2 bg-primary-red
              "></div></div></a><a style="pointer-events:none" class="ml-8 text-lg font-bold tracking-wider h-full " href="/joining-us"><div class="relative h-full flex items-center justify-center group"><div class="">Joining us</div><div class="
                w-0 group-hover:w-[70px]
                transition-all duration-300 ease-in-out
               h-[5px] absolute bottom-0 left-1/2 -translate-x-1/2 bg-primary-red
              "></div></div></a></div><button class="md:hidden flex items-center cursor-pointer justify-center w-10 h-10"><img alt="Open Menu" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" class="text-primary-red" style="color:transparent" src="/_next/static/media/menu.71926c06.svg"/></button></div></div><div class="bg-white fixed top-[60px]
      h-0 opacity-0 
      transition-all duration-300 ease-in-out overflow-y-hidden
      w-full z-[999] 
    "><div class="flex-col flex items-start w-full"><button style="pointer-events:auto" class="ml-8 text-xl font-bold tracking-wider h-full "><div class="relative h-full flex gap-4 items-center justify-center group py-[13px]"><div class="
                w-0 group-hover:h-0
                transition-all duration-300 ease-in-out
               w-[3px]   bg-primary-red
              "></div><div class="">Home</div></div></button><button style="pointer-events:auto" class="ml-8 text-xl font-bold tracking-wider h-full "><div class="relative h-full flex gap-4 items-center justify-center group py-[13px]"><div class="
                w-0 group-hover:h-0
                transition-all duration-300 ease-in-out
               w-[3px]   bg-primary-red
              "></div><div class="">Members</div></div></button><button style="pointer-events:auto" class="ml-8 text-xl font-bold tracking-wider h-full "><div class="relative h-full flex gap-4 items-center justify-center group py-[13px]"><div class="
                h-[20px] text-primary-red
                transition-all duration-300 ease-in-out
               w-[3px]   bg-primary-red
              "></div><div class=" text-primary-red">Publications</div></div></button><button style="pointer-events:none" class="ml-8 text-xl font-bold tracking-wider h-full "><div class="relative h-full flex gap-4 items-center justify-center group py-[13px]"><div class="
                w-0 group-hover:h-0
                transition-all duration-300 ease-in-out
               w-[3px]   bg-primary-red
              "></div><div class="">Joining us</div></div></button></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div>Loading...</div><!--/$--><!--$--><!--/$--><div class="w-full relative flex justify-center "><div class=" w-full xl:max-w-[1140px] 2xl:max-w-[1320px] lg:max-w-[960px] md:max-w-[720px] sm:max-w-[540px] px-4 md:px-0 relative "><div class="w-full h-[1px] bg-gray-300 my-4"></div><div class="mb-10"><div class="flex items-end justify-start gap-2"><img alt="CBIM Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="" style="color:transparent" srcSet="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=48&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=96&amp;q=75 2x" src="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.45f27292.png&amp;w=96&amp;q=75"/><div class="border-l-2 pl-2  text-3xl font-bold leading-none">CBIM</div></div></div></div></div><script src="/_next/static/chunks/webpack-d36f1d6244358adb.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[5379,[\"850\",\"static/chunks/850-de350858a22819d2.js\",\"177\",\"static/chunks/app/layout-30653af3c87d6211.js\"],\"ThemeProvider\"]\n3:I[1945,[\"850\",\"static/chunks/850-de350858a22819d2.js\",\"177\",\"static/chunks/app/layout-30653af3c87d6211.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\n7:I[4431,[],\"OutletBoundary\"]\n9:I[5278,[],\"AsyncMetadataOutlet\"]\nb:I[4431,[],\"ViewportBoundary\"]\nd:I[4431,[],\"MetadataBoundary\"]\ne:\"$Sreact.suspense\"\n10:I[7150,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/886f446b96dc7734-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/9cf9c6e84ed13b5e-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/e693e841d50dcf2f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/a7d7cda7084e7e71.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"9hS1bw4vLqB3Hrxjps2LV\",\"p\":\"\",\"c\":[\"\",\"publications\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a7d7cda7084e7e71.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_188709 __variable_9a8899 __variable_7dbc8c __className_e73cbf antialiased relative\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]}]]}],{\"children\":[\"publications\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",null,[\"$\",\"$L7\",null,{\"children\":[\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$Lf\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:I[622,[],\"IconMark\"]\na:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Create Next App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"$L11\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"f:\"$a:metadata\"\n"])</script><script>self.__next_f.push([1,"12:I[4057,[\"850\",\"static/chunks/850-de350858a22819d2.js\",\"352\",\"static/chunks/app/publications/page-b592bbbaf126a7f6.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"$L12\",null,{\"publications\":[{\"year\":2023,\"papers\":[{\"title\":\"Score-Guided Diffusion for 3D Human Recovery\",\"authors\":[\"A Stathopoulos\",\"L Han\",\"D Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"FdaFVdcAAAAJ:8k81kl-MbHgC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=FdaFVdcAAAAJ\u0026pagesize=100\u0026citation_for_view=FdaFVdcAAAAJ:8k81kl-MbHgC\",\"publisher\":\"Computer Vision and Pattern Recognition (CVPR), 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"ProxEdit: Improving Tuning-Free Real Image Editing With Proximal Guidance\",\"authors\":[\"L Han\",\"S Wen\",\"Q Chen\",\"Z Zhang\",\"K Song\",\"M Ren\",\"R Gao\",\"A Stathopoulos\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"FdaFVdcAAAAJ:0EnyYjriUFMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=FdaFVdcAAAAJ\u0026pagesize=100\u0026citation_for_view=FdaFVdcAAAAJ:0EnyYjriUFMC\",\"publisher\":\"Winter Conference on Applications of Computer Vision (WACV), 2024, 2024\",\"venue\":\"WACV\"},{\"title\":\"Layout-agnostic scene text image synthesis with diffusion models\",\"authors\":[\"Q Zhangli\",\"J Jiang\",\"D Liu\",\"L Yu\",\"X Dai\",\"A Ramchandani\",\"G Pang\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"sLxp0xAAAAAJ:Y0pCki6q_DkC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=sLxp0xAAAAAJ\u0026pagesize=100\u0026citation_for_view=sLxp0xAAAAAJ:Y0pCki6q_DkC\",\"publisher\":\"2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR …, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Implicit In-context Learning\",\"authors\":[\"Z Li\",\"Z Xu\",\"L Han\",\"Y Gao\",\"S Wen\",\"D Liu\",\"H Wang\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"TOsFPu4AAAAJ:0EnyYjriUFMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=TOsFPu4AAAAJ\u0026pagesize=100\u0026citation_for_view=TOsFPu4AAAAJ:0EnyYjriUFMC\",\"publisher\":\"The Thirteenth International Conference on Learning Representations (ICLR), 2025, 2024\",\"venue\":\"ICLR\"},{\"title\":\"SINE: Single image editing with text-to-image diffusion models\",\"authors\":[\"Z Zhang\",\"L Han\",\"A Ghosh\",\"DN Metaxas\",\"J Ren\"],\"date\":\"2023-01-01\",\"id\":\"RhM5qHoAAAAJ:IjCSPb-OGe4C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:IjCSPb-OGe4C\",\"publisher\":\"CVPR 2023, 2023\",\"venue\":\"CVPR\"},{\"title\":\"Omnilabel: A challenging benchmark for language-based object detection\",\"authors\":[\"S Schulter\",\"Y Suh\",\"KM Dafnis\",\"Z Zhang\",\"S Zhao\",\"D Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"RhM5qHoAAAAJ:UebtZRa9Y70C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:UebtZRa9Y70C\",\"publisher\":\"ICCV 2023, 2023\",\"venue\":\"ICCV\"},{\"title\":\"OmniLabel: A Challenging Benchmark for Language-Based Object Detection\",\"authors\":[\"S Schulter\",\"VK B G\",\"Y Suh\",\"KM Dafnis\",\"Z Zhang\",\"S Zhao\",\"D Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"M-qzlU8AAAAJ:ufrVoPGSRksC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=M-qzlU8AAAAJ\u0026pagesize=100\u0026citation_for_view=M-qzlU8AAAAJ:ufrVoPGSRksC\",\"publisher\":\"ICCV 2023, 2023\",\"venue\":\"ICCV\"},{\"title\":\"LEPARD: Learning Explicit Part Discovery for 3D Articulated Shape Reconstruction\",\"authors\":[\"D Liu\",\"Q Zhangli\",\"Y Gao\",\"DN Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"1uo3XsMAAAAJ:8k81kl-MbHgC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:8k81kl-MbHgC\",\"publisher\":\"NeurIPS 2023, 2023\",\"venue\":\"NeurIPS\"},{\"title\":\"Deformer: Integrating transformers with deformable models for 3d shape abstraction from a single image\",\"authors\":[\"D Liu\",\"X Yu\",\"M Ye\",\"Q Zhangli\",\"Z Li\",\"Z Zhang\",\"DN Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"RhM5qHoAAAAJ:LkGwnXOMwfcC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:LkGwnXOMwfcC\",\"publisher\":\"ICCV 2023, 2023\",\"venue\":\"ICCV\"},{\"title\":\"Deformer: Integrating transformers with deformable models for 3d shape abstraction from a single image\",\"authors\":[\"D Liu\",\"X Yu\",\"M Ye\",\"Q Zhangli\",\"Z Li\",\"Z Zhang\",\"DN Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"51OJEPcAAAAJ:Tyk-4Ss8FVUC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:Tyk-4Ss8FVUC\",\"publisher\":\"IEEE/CVF International Conference on Computer Vision (ICCV 2023), 2023\",\"venue\":\"ICCV\"},{\"title\":\"DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image\",\"authors\":[\"D Liu\",\"X Yu\",\"M Ye\",\"Q Zhangli\",\"Z Li\",\"Z Zhang\",\"DN Metaxas\"],\"date\":\"2023-01-01\",\"id\":\"1uo3XsMAAAAJ:KlAtU1dfN6UC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:KlAtU1dfN6UC\",\"publisher\":\"ICCV 2023, 2023\",\"venue\":\"ICCV\"}]},{\"year\":2024,\"papers\":[{\"title\":\"SODA: Spectral Orthogonal Decomposition Adaptation for Diffusion Models\",\"authors\":[\"X Zhang\",\"S Wen\",\"L Han\",\"F Juefei-Xu\",\"A Srivastava\",\"J Huang\",\"V Pavlovic\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"yTQbz6AAAAAJ:mVmsd5A6BfQC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=yTQbz6AAAAAJ\u0026pagesize=100\u0026citation_for_view=yTQbz6AAAAAJ:mVmsd5A6BfQC\",\"publisher\":\"2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV …, 2025\",\"venue\":\"WACV\"},{\"title\":\"SODA: Spectral Orthogonal Decomposition Adaptation for Diffusion Models\",\"authors\":[\"X Zhang*\",\"S Wen*\",\"L Han*†\",\"F Juefei-Xu\",\"A Srivastava\",\"J Huang\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"n2v43R4AAAAJ:35N4QoGY0k4C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=n2v43R4AAAAJ\u0026pagesize=100\u0026citation_for_view=n2v43R4AAAAJ:35N4QoGY0k4C\",\"publisher\":\"2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV …, 2025\",\"venue\":\"WACV\"},{\"title\":\"SF-V: Single forward video generation model\",\"authors\":[\"Z Zhang\",\"Y Li\",\"Y Wu\",\"A Kag\",\"I Skorokhodov\",\"W Menapace\",\"A Siarohin\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"RhM5qHoAAAAJ:9ZlFYXVOiuMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:9ZlFYXVOiuMC\",\"publisher\":\"NeurIPS 2024, 2025\",\"venue\":\"NeurIPS\"},{\"title\":\"K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model\",\"authors\":[\"B Guo\",\"Y Gao\",\"M Ye\",\"D Gu\",\"Y Zhou\",\"L Axel\",\"D Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"TOsFPu4AAAAJ:7PzlFSSx8tAC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=TOsFPu4AAAAJ\u0026pagesize=100\u0026citation_for_view=TOsFPu4AAAAJ:7PzlFSSx8tAC\",\"publisher\":\"The Fourteenth International Conference on Learning Representations (ICLR), 2026, 2025\",\"venue\":\"ICLR\"},{\"title\":\"Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI Segmentation\",\"authors\":[\"M Ye\",\"B Xin\",\"L Axel\",\"D Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"iYBbF7QAAAAJ:LkGwnXOMwfcC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=iYBbF7QAAAAJ\u0026pagesize=100\u0026citation_for_view=iYBbF7QAAAAJ:LkGwnXOMwfcC\",\"publisher\":\"2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV …, 2025\",\"venue\":\"WACV\"},{\"title\":\"Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation\",\"authors\":[\"Y Gao\",\"Z Li\",\"D Liu\",\"M Zhou\",\"S Zhang\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:UebtZRa9Y70C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:UebtZRa9Y70C\",\"publisher\":\"CVPR 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Taming self-training for open-vocabulary object detection\",\"authors\":[\"S Zhao\",\"S Schulter\",\"L Zhao\",\"Z Zhang\",\"Y Suh\",\"M Chandraker\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"RhM5qHoAAAAJ:7PzlFSSx8tAC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:7PzlFSSx8tAC\",\"publisher\":\"CVPR 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Steering prototypes with prompt-tuning for rehearsal-free continual learning\",\"authors\":[\"Z Li\",\"L Zhao\",\"Z Zhang\",\"H Zhang\",\"D Liu\",\"T Liu\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:ufrVoPGSRksC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:ufrVoPGSRksC\",\"publisher\":\"WACV 2024, 2024\",\"venue\":\"WACV\"},{\"title\":\"Steering prototypes with prompt-tuning for rehearsal-free continual learning\",\"authors\":[\"Z Li\",\"L Zhao\",\"Z Zhang\",\"H Zhang\",\"D Liu\",\"T Liu\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"51OJEPcAAAAJ:IjCSPb-OGe4C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:IjCSPb-OGe4C\",\"publisher\":\"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024 …, 2024\",\"venue\":\"WACV\"},{\"title\":\"Second-order graph odes for multi-agent trajectory forecasting\",\"authors\":[\"S Wen\",\"H Wang\",\"D Liu\",\"Q Zhangli\",\"D Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:Zph67rFs4hoC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:Zph67rFs4hoC\",\"publisher\":\"WACV 2024, 2024\",\"venue\":\"WACV\"},{\"title\":\"Proxedit: Improving tuning-free real image editing with proximal guidance\",\"authors\":[\"L Han\",\"S Wen\",\"Q Chen\",\"Z Zhang\",\"K Song\",\"M Ren\",\"R Gao\",\"A Stathopoulos\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:YOwf2qJgpHMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:YOwf2qJgpHMC\",\"publisher\":\"WACV 2024, 2024\",\"venue\":\"WACV\"},{\"title\":\"Proxedit: Improving tuning-free real image editing with proximal guidance\",\"authors\":[\"L Han\",\"S Wen\",\"Q Chen\",\"Z Zhang\",\"K Song\",\"M Ren\",\"R Gao\",\"A Stathopoulos\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"RhM5qHoAAAAJ:MXK_kJrjxJIC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:MXK_kJrjxJIC\",\"publisher\":\"WACV 2024, 2024\",\"venue\":\"WACV\"},{\"title\":\"Layout-Agnostic Scene Text Image Synthesis with Diffusion Models\",\"authors\":[\"Q Zhangli\",\"J Jiang\",\"D Liu\",\"L Yu\",\"X Dai\",\"A Ramchandani\",\"G Pang\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:qxL8FJ1GzNcC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:qxL8FJ1GzNcC\",\"publisher\":\"CVPR 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Instantaneous Perception of Moving Objects in 3D\",\"authors\":[\"D Liu\",\"B Zhuang\",\"DN Metaxas\",\"M Chandraker\"],\"date\":\"2024-01-01\",\"id\":\"1uo3XsMAAAAJ:4TOpqqG69KYC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:4TOpqqG69KYC\",\"publisher\":\"CVPR 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection\",\"authors\":[\"M Pan\",\"Z Wang\",\"X Dong\",\"V Sehwag\",\"L Lyu\",\"X Lin\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:0EnyYjriUFMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:0EnyYjriUFMC\",\"publisher\":\"The 18th European Conference on Computer Vision (ECCV 2024), 2024\",\"venue\":\"ECCV\"},{\"title\":\"AVID: Any-length video inpainting with diffusion model\",\"authors\":[\"Z Zhang\",\"B Wu\",\"X Wang\",\"Y Luo\",\"L Zhang\",\"Y Zhao\",\"P Vajda\",\"D Metaxas\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"RhM5qHoAAAAJ:dhFuZR0502QC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:dhFuZR0502QC\",\"publisher\":\"CVPR 2024, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Training like a medical resident: universal medical image segmentation via context prior learning\",\"authors\":[\"Y Gao\",\"Z Li\",\"D Liu\",\"M Zhou\",\"S Zhang\",\"DN Meta\"],\"date\":\"2023-01-01\",\"id\":\"51OJEPcAAAAJ:zYLM7Y9cAGgC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:zYLM7Y9cAGgC\",\"publisher\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024), 2023\",\"venue\":\"CVPR\"}]},{\"year\":2025,\"papers\":[{\"title\":\"The hidden life of tokens: Reducing hallucination of large vision-language models via visual information steering\",\"authors\":[\"Z Li\",\"H Shi\",\"Y Gao\",\"D Liu\",\"Z Wang\",\"Y Chen\",\"T Liu\",\"L Zhao\",\"H Wang\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"51OJEPcAAAAJ:ufrVoPGSRksC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:ufrVoPGSRksC\",\"publisher\":\"International Conference on Machine Learning (ICML 2025), 2025\",\"venue\":\"ICML\"},{\"title\":\"The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering\",\"authors\":[\"Z Li\",\"H Shi\",\"Y Gao\",\"D Liu\",\"Z Wang\",\"Y Chen\",\"T Liu\",\"L Zhao\",\"H Wang\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"QSYVbj8AAAAJ:maZDTaKrznsC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:maZDTaKrznsC\",\"publisher\":\"ICML 2025, 2025\",\"venue\":\"ICML\"},{\"title\":\"The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering\",\"authors\":[\"Z Li\",\"H Shi\",\"Y Gao\",\"D Liu\",\"Z Wang\",\"Y Chen\",\"T Liu\",\"L Zhao\",\"H Wang\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:hC7cP41nSMkC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:hC7cP41nSMkC\",\"publisher\":\"ICML 2025, 2025\",\"venue\":\"ICML\"},{\"title\":\"T2Bs: Text-to-Character Blendshapes via Video Generation\",\"authors\":[\"J Luo\",\"C Wang\",\"M Vasilkovsky\",\"V Shakhrai\",\"D Liu\",\"P Zhuang\",\"S Tulyakov\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:TFP_iSt0sucC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:TFP_iSt0sucC\",\"publisher\":\"ICCV 2025, 2025\",\"venue\":\"ICCV\"},{\"title\":\"Snapgen-v: Generating a five-second video within five seconds on a mobile device\",\"authors\":[\"Y Wu*\",\"Z Zhang*\",\"Y Li*\",\"Y Xu\",\"A Kag\",\"Y Sui\",\"H Coskun\",\"K Ma\",\"A Lebedev\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"RhM5qHoAAAAJ:ZeXyd9-uunAC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=RhM5qHoAAAAJ\u0026pagesize=100\u0026citation_for_view=RhM5qHoAAAAJ:ZeXyd9-uunAC\",\"publisher\":\"CVPR 2025, 2025\",\"venue\":\"CVPR\"},{\"title\":\"Show and Segment: Universal Medical Image Segmentation via In-Context Learning\",\"authors\":[\"Y Gao\",\"D Liu\",\"Z Li\",\"Y Li\",\"D Chen\",\"M Zhou\",\"DN Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:mB3voiENLucC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:mB3voiENLucC\",\"publisher\":\"CVPR 2025, 2025\",\"venue\":\"CVPR\"},{\"title\":\"Show and Segment: Universal Medical Image Segmentation via In-Context Learning\",\"authors\":[\"Y Gao\",\"D Liu\",\"Z Li\",\"Y Li\",\"D Chen\",\"M Zhou\",\"DN Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"51OJEPcAAAAJ:hqOjcs7Dif8C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:hqOjcs7Dif8C\",\"publisher\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025), 2025\",\"venue\":\"CVPR\"},{\"title\":\"Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing\",\"authors\":[\"Z Li\",\"T Zhao\",\"X Xu\",\"Z Zhang\",\"Z Li\",\"X Chen\",\"Q Zhang\",\"A Bergamo\",\"AK Jain\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"51OJEPcAAAAJ:5nxA0vEk-isC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:5nxA0vEk-isC\",\"publisher\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025), 2025\",\"venue\":\"CVPR\"},{\"title\":\"LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation\",\"authors\":[\"C Jin\",\"Y Li\",\"M Zhao\",\"S Zhao\",\"Z Wang\",\"X He\",\"L Han\",\"T Che\",\"DN Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"QSYVbj8AAAAJ:k_IJM867U9cC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:k_IJM867U9cC\",\"publisher\":\"ICLR 2025, 2025\",\"venue\":\"ICLR\"},{\"title\":\"LUCAS: Layered Universal Codec Avatars\",\"authors\":[\"D Liu\",\"T Deng\",\"G Nam\",\"Y Rong\",\"S Pidhorskyi\",\"J Li\",\"J Saragih\",\"DN Metaxas\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:_Qo2XoVZTnwC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:_Qo2XoVZTnwC\",\"publisher\":\"CVPR 2025, 2025\",\"venue\":\"CVPR\"},{\"title\":\"Invisible Backdoor Attack against Self-supervised Learning\",\"authors\":[\"H Zhang*\",\"Z Wang*\",\"B Li\",\"F Lin\",\"T Han\",\"M Jin\",\"C Zhan\",\"M Du\",\"H Wang\",\"...\"],\"date\":\"2025-01-01\",\"id\":\"QSYVbj8AAAAJ:J_g5lzvAfSwC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:J_g5lzvAfSwC\",\"publisher\":\"CVPR 2025 (* indicates equal contributions), 25790-25801, 2025\",\"venue\":\"CVPR\"},{\"title\":\"Improved Training Technique for Latent Consistency Models\",\"authors\":[\"Q Dao\",\"K Doan\",\"D Liu\",\"T Le\",\"D Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:qUcmZB5y_30C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:qUcmZB5y_30C\",\"publisher\":\"ICLR 2025, 2025\",\"venue\":\"ICLR\"},{\"title\":\"Implicit In-context Learning\",\"authors\":[\"Z Li\",\"Z Xu\",\"L Han\",\"Y Gao\",\"S Wen\",\"D Liu\",\"H Wang\",\"DN Metaxas\"],\"date\":\"2025-01-01\",\"id\":\"1uo3XsMAAAAJ:4DMP91E08xMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=1uo3XsMAAAAJ\u0026pagesize=100\u0026citation_for_view=1uo3XsMAAAAJ:4DMP91E08xMC\",\"publisher\":\"ICLR 2025, 2025\",\"venue\":\"ICLR\"},{\"title\":\"Continuous Concepts Removal in Text-to-image Diffusion Models\",\"authors\":[\"T Han\",\"W Sun\",\"Y Hu\",\"C Fang\",\"Y Zhang\",\"S Ma\",\"T Zheng\",\"Z Chen\",\"Z Wang\"],\"date\":\"2025-01-01\",\"id\":\"QSYVbj8AAAAJ:4JMBOYKVnBMC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:4JMBOYKVnBMC\",\"publisher\":\"NeurIPS 2025, 2025\",\"venue\":\"NeurIPS\"},{\"title\":\"CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI\",\"authors\":[\"S Cheng\",\"L Lyu\",\"Z Wang\",\"X Zhang\",\"V Sehwag\"],\"date\":\"2025-01-01\",\"id\":\"QSYVbj8AAAAJ:GnPB-g6toBAC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:GnPB-g6toBAC\",\"publisher\":\"CVPR 2025, 2025\",\"venue\":\"CVPR\"},{\"title\":\"Visual Agents as Fast and Slow Thinkers\",\"authors\":[\"G Sun\",\"M Jin\",\"Z Wang\",\"CL Wang\",\"S Ma\",\"Q Wang\",\"YN Wu\",\"Y Zhang\",\"D Liu\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:Wp0gIr-vW9MC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:Wp0gIr-vW9MC\",\"publisher\":\"ICLR 2025, 2024\",\"venue\":\"ICLR\"},{\"title\":\"ProSec: Fortifying Code LLMs with Proactive Security Alignment\",\"authors\":[\"X Xu\",\"Z Su\",\"J Guo\",\"K Zhang\",\"Z Wang\",\"X Zhang\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:HDshCWvjkbEC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:HDshCWvjkbEC\",\"publisher\":\"ICML 2025, 2024\",\"venue\":\"ICML\"},{\"title\":\"MLLM-as-a-Judge for Image Safety without Human Labeling\",\"authors\":[\"Z Wang\",\"S Hu\",\"S Zhao\",\"X Lin\",\"F Juefei-Xu\",\"Z Li\",\"L Han\",\"H Subramanyam\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:bEWYMUwI8FkC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:bEWYMUwI8FkC\",\"publisher\":\"CVPR 2025, 2024\",\"venue\":\"CVPR\"},{\"title\":\"MLLM-as-a-Judge for Image Safety without Human Labeling\",\"authors\":[\"Z Wang\",\"S Hu\",\"S Zhao\",\"X Lin\",\"F Juefei-Xu\",\"Z Li\",\"L Han\",\"H Subramanyam\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"M-qzlU8AAAAJ:kNdYIx-mwKoC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=M-qzlU8AAAAJ\u0026pagesize=100\u0026citation_for_view=M-qzlU8AAAAJ:kNdYIx-mwKoC\",\"publisher\":\"CVPR 2025, 2024\",\"venue\":\"CVPR\"},{\"title\":\"MLLM-as-a-Judge for Image Safety without Human Labeling\",\"authors\":[\"Z Wang\",\"S Hu\",\"S Zhao\",\"X Lin\",\"F Juefei-Xu\",\"Z Li\",\"L Han\",\"H Subramanyam\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"51OJEPcAAAAJ:WF5omc3nYNoC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:WF5omc3nYNoC\",\"publisher\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025), 2024\",\"venue\":\"CVPR\"},{\"title\":\"Implicit In-context Learning\",\"authors\":[\"Z Li\",\"Z Xu\",\"L Han\",\"Y Gao\",\"S Wen\",\"D Liu\",\"H Wang\",\"DN Metaxas\"],\"date\":\"2024-01-01\",\"id\":\"51OJEPcAAAAJ:UebtZRa9Y70C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=51OJEPcAAAAJ\u0026pagesize=100\u0026citation_for_view=51OJEPcAAAAJ:UebtZRa9Y70C\",\"publisher\":\"The Thirteenth International Conference on Learning Representations (ICLR 2025), 2024\",\"venue\":\"ICLR\"},{\"title\":\"Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents\",\"authors\":[\"H Zhang\",\"J Huang\",\"K Mei\",\"Y Yao\",\"Z Wang\",\"C Zhan\",\"H Wang\",\"Y Zhang\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:IWHjjKOFINEC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:IWHjjKOFINEC\",\"publisher\":\"ICLR 2025, 2024\",\"venue\":\"ICLR\"},{\"title\":\"Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction\",\"authors\":[\"S Zhao\",\"Z Wang\",\"F Juefei-Xu\",\"X Xia\",\"M Liu\",\"X Wang\",\"M Liang\",\"N Zhang\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"QSYVbj8AAAAJ:j3f4tGmQtD8C\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=QSYVbj8AAAAJ\u0026pagesize=100\u0026citation_for_view=QSYVbj8AAAAJ:j3f4tGmQtD8C\",\"publisher\":\"CVPR 2025, 2024\",\"venue\":\"CVPR\"},{\"title\":\"Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction\",\"authors\":[\"S Zhao\",\"Z Wang\",\"F Juefei-Xu\",\"X Xia\",\"M Liu\",\"X Wang\",\"M Liang\",\"N Zhang\",\"...\"],\"date\":\"2024-01-01\",\"id\":\"M-qzlU8AAAAJ:MXK_kJrjxJIC\",\"link\":\"https://scholar.google.com/citations?view_op=view_citation\u0026hl=en\u0026user=M-qzlU8AAAAJ\u0026pagesize=100\u0026citation_for_view=M-qzlU8AAAAJ:MXK_kJrjxJIC\",\"publisher\":\"CVPR 2025, 2024\",\"venue\":\"CVPR\"}]}]}]\n"])</script></body></html>